{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYbvvoQNjUCGeiPSd/GQyz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.\n","\n","I recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is here:\n","\n","https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing\n","\n","https://github.com/karpathy/makemore\n","\n","https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb\n","\n","https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing\n","\n","## Reading\n","https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\n","\n","[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n","\n","[Bessel's Correction](https://math.oxford.emory.edu/site/math117/besselCorrection/)\n","\n","[A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"],"metadata":{"id":"x6ssJK7SGkMQ"}},{"cell_type":"code","execution_count":33,"metadata":{"id":"xKLSvNbYGNT3","executionInfo":{"status":"ok","timestamp":1753046572491,"user_tz":240,"elapsed":16,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt # for making figures\n","%matplotlib inline"]},{"cell_type":"code","source":["# download the names.txt file from github\n","!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDK4ZuEW5FaD","executionInfo":{"status":"ok","timestamp":1753046572696,"user_tz":240,"elapsed":209,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"f704ec10-9e95-4c7c-fb70-692a7e1c4137"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-07-20 21:22:52--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 228145 (223K) [text/plain]\n","Saving to: ‘names.txt.1’\n","\n","\rnames.txt.1           0%[                    ]       0  --.-KB/s               \rnames.txt.1         100%[===================>] 222.80K  --.-KB/s    in 0.04s   \n","\n","2025-07-20 21:22:52 (6.12 MB/s) - ‘names.txt.1’ saved [228145/228145]\n","\n"]}]},{"cell_type":"code","source":["# read in all the words\n","words = open('names.txt', 'r').read().splitlines()\n","print(len(words))\n","print(max(len(w) for w in words))\n","print(words[:8])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWF8WE8P5Idc","executionInfo":{"status":"ok","timestamp":1753046572754,"user_tz":240,"elapsed":57,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"a869cf16-181f-4dd6-a0f6-492961f9877d"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["32033\n","15\n","['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"]}]},{"cell_type":"code","source":["# build the vocabulary of characters and mappings to/from integers\n","chars = sorted(list(set(''.join(words))))\n","stoi = {s:i+1 for i,s in enumerate(chars)}\n","stoi['.'] = 0\n","itos = {i:s for s,i in stoi.items()}\n","vocab_size = len(itos)\n","print(itos)\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-PCQGcj75LNy","executionInfo":{"status":"ok","timestamp":1753046572754,"user_tz":240,"elapsed":15,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"e86a4556-7e78-4537-de6e-4a0a5e758ab1"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n","27\n"]}]},{"cell_type":"code","source":["# build the dataset\n","block_size = 3 # context length: how many characters do we take to predict the next one?\n","\n","def build_dataset(words):\n","  X, Y = [], []\n","\n","  for w in words:\n","    context = [0] * block_size\n","    for ch in w + '.':\n","      ix = stoi[ch]\n","      X.append(context)\n","      Y.append(ix)\n","      context = context[1:] + [ix] # crop and append\n","\n","  X = torch.tensor(X)\n","  Y = torch.tensor(Y)\n","  print(X.shape, Y.shape)\n","  return X, Y\n","\n","import random\n","random.seed(42)\n","random.shuffle(words)\n","n1 = int(0.8*len(words))\n","n2 = int(0.9*len(words))\n","\n","Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n","Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n","Xte,  Yte  = build_dataset(words[n2:])     # 10%"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uy6Qo3hb5N3W","executionInfo":{"status":"ok","timestamp":1753046573871,"user_tz":240,"elapsed":1119,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"d37fee9c-cc3a-48f1-83e0-131f205f9163"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([182625, 3]) torch.Size([182625])\n","torch.Size([22655, 3]) torch.Size([22655])\n","torch.Size([22866, 3]) torch.Size([22866])\n"]}]},{"cell_type":"code","source":["# ok biolerplate done, now we get to the action:"],"metadata":{"id":"MDHfpbPM5TgI","executionInfo":{"status":"ok","timestamp":1753046573872,"user_tz":240,"elapsed":15,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# utility function we will use later when comparing manual gradients to PyTorch gradients\n","def cmp(s, dt, t):\n","  ex = torch.all(dt == t.grad).item()\n","  app = torch.allclose(dt, t.grad)\n","  maxdiff = (dt - t.grad).abs().max().item()\n","  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"],"metadata":{"id":"2m7bLyf85UM4","executionInfo":{"status":"ok","timestamp":1753046573873,"user_tz":240,"elapsed":7,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","# Note: I am initializating many of these parameters in non-standard ways\n","# because sometimes initializating with e.g. all zeros could mask an incorrect\n","# implementation of the backward pass.\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQr2OOkj5Wyf","executionInfo":{"status":"ok","timestamp":1753046573884,"user_tz":240,"elapsed":15,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"8dc8144d-9f63-46f1-b8ca-5ec650036bdd"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["4137\n"]}]},{"cell_type":"code","source":["batch_size = 32\n","n = batch_size # a shorter variable also, for convenience\n","# construct a minibatch\n","ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"],"metadata":{"id":"vDVwnUiO5a0B","executionInfo":{"status":"ok","timestamp":1753046573930,"user_tz":240,"elapsed":34,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n","\n","emb = C[Xb] # embed the characters into vectors\n","embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","# Linear layer 1\n","hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n","# BatchNorm layer\n","bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","bndiff = hprebn - bnmeani\n","bndiff2 = bndiff**2\n","bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","bnvar_inv = (bnvar + 1e-5)**-0.5\n","bnraw = bndiff * bnvar_inv\n","hpreact = bngain * bnraw + bnbias\n","# Non-linearity\n","h = torch.tanh(hpreact) # hidden layer\n","# Linear layer 2\n","logits = h @ W2 + b2 # output layer\n","# cross entropy loss (same as F.cross_entropy(logits, Yb))\n","logit_maxes = logits.max(1, keepdim=True).values\n","norm_logits = logits - logit_maxes # subtract max for numerical stability\n","counts = norm_logits.exp()\n","counts_sum = counts.sum(1, keepdims=True)\n","counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","# there are 2 operations here:\n","# 1. replication (see shapes, they are different)\n","# 2. multiplication\n","probs = counts * counts_sum_inv\n","logprobs = probs.log()\n","loss = -logprobs[range(n), Yb].mean()\n","\n","# PyTorch backward pass\n","for p in parameters:\n","  p.grad = None\n","for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n","          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n","         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n","         embcat, emb]:\n","  t.retain_grad()\n","loss.backward()\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xDmQ8N7W5hFz","executionInfo":{"status":"ok","timestamp":1753046574022,"user_tz":240,"elapsed":98,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"8df2981b-a6a5-4272-a018-62872a18ff8c"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.3265, grad_fn=<NegBackward0>)"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["logprobs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvvENRXbA-6B","executionInfo":{"status":"ok","timestamp":1753046574022,"user_tz":240,"elapsed":6,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"7de7e709-f92c-4312-fd92-f81342692643"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.5639, -2.3885, -4.0014, -2.9685, -3.8704, -2.4131, -3.6893, -3.3901,\n","        -4.1723, -3.3830, -3.3327, -3.2747, -3.4310, -3.6483, -3.3089, -4.2985,\n","        -4.6638, -3.9426, -4.2326, -3.0784, -2.9797, -3.8206, -3.7519, -2.6625,\n","        -2.8387, -3.6343, -3.8724], grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["logprobs.shape\n","t1 = torch.zeros(logprobs.shape)\n","t1.shape\n","probs.shape\n","# different shape of counts!\n","counts.shape, counts_sum_inv.shape\n","bngain.shape, bnraw.shape, bnbias.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HUhdTfgT-5pK","executionInfo":{"status":"ok","timestamp":1753046574036,"user_tz":240,"elapsed":16,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"df67b7ed-ed52-4557-8948-283a5a56afcd"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# Exercise 1: backprop through the whole thing manually,\n","# backpropagating through exactly all of the variables\n","# as they are defined in the forward pass above, one by one\n","\n","# https://docs.pytorch.org/docs/stable/generated/torch.zeros_like.html\n","# Returns a tensor filled with the scalar value 0, with the same size as input\n","dlogprobs = torch.zeros(logprobs.shape)\n","# identical way to do the same\n","# dlogprobs = torch.zeros_like(logprobs)\n","# loss = -(a + b + c) /3\n","# dloss = -1/3a + -1/3b + -1/3c\n","# dloss/da = -1/n\n","# the loss of the rest elements in logprobs is 0, because they don't participate in calculation of loss\n","dlogprobs[range(n), Yb] = -1.0/n\n","\n","# no need to init dprobs, it will be created from actual calculation\n","# dprobs = torch.zeros_like(probs)\n","# derivative of d/dxlog(x) = 1/x\n","dprobs = (1.0 / probs) * dlogprobs # *dlogprobs due to the chain rule\n","\n","# derivative of d/da (a*b) = b\n","# in our case derivative is counts\n","dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # chain rule\n","\n","dcounts = counts_sum_inv * dprobs # chain rule\n","# d/dx(1/x) = -1/x^2\n","dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv # chain rule\n","# shapes are different\n","# counts_sum = counts.sum(1, keepdims=True)\n","# also! we already calculate dcounts and we need to += add it to the previous one\n","dcounts += torch.ones_like(counts) * dcounts_sum # basically we replicate it 27 times\n","# we check dcounts later, because it depends on counts_sum_inv\n","\n","# d/dx(e^x) = e^x\n","dnorm_logits = counts * dcounts\n","\n","# logit_maxes and logits have different shape!\n","# d/db(a - b) = -1\n","dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n","\n","# d/da(a - b) = 1\n","# just a copy of dnorm_logits\n","dlogits = dnorm_logits.clone()\n","# could not find this solution by myself\n","# can be done like we did dlogprobs\n","dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n","\n","# could not figure it out by myself :/\n","# T = transposed matrix => check their shapes\n","dh = dlogits @ W2.T\n","dW2 = h.T @ dlogits\n","db2 = dlogits.sum(0)\n","\n","# da/dx tanh(x) = 1 - a^2\n","dhpreact = (1.0 - h**2) * dh\n","\n","# due to the different shape\n","dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n","dbnraw = bngain * dhpreact\n","dbnbias = (dhpreact).sum(0, keepdim=True)\n","\n","dbndiff = bnvar_inv * dbnraw\n","dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)\n","# d/dx(x^n) = n x^(n - 1)\n","dbnvar = - 0.5 * (bnvar + 1e-5) ** -1.5 * dbnvar_inv\n","dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff) * dbnvar\n","# d/dx(x^2) = 2 x\n","dbndiff += (2*bndiff) * dbndiff2\n","# d/dx(a - x) = -1\n","dbnmeani = -dbndiff.sum(0)\n","# d/da(a - x) = 1\n","dhprebn = dbndiff.clone()\n","dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n","#\n","dembcat = dhprebn @ W1.T\n","dW1 = embcat.T @ dhprebn\n","db1 = dhprebn.sum(0)\n","# undo shape changing\n","demb = dembcat.view(emb.shape)\n","# ----------------------------\n","# emb = C[Xb]\n","# most complicated for me :/\n","print(\"----------------------------\")\n","print(emb.shape, C.shape, Xb.shape)\n","print(\"----------------------------\")\n","dC = torch.zeros_like(C)\n","for k in range(Xb.shape[0]):\n","  for j in range(Xb.shape[1]):\n","    ix = Xb[k,j]\n","    dC[ix] += demb[k,j]\n","\n","# https://youtu.be/q8SA3rM6ckI?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\n","# 1:32:00\n","cmp('logprobs', dlogprobs, logprobs)\n","cmp('probs', dprobs, probs)\n","cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n","cmp('counts_sum', dcounts_sum, counts_sum)\n","cmp('counts', dcounts, counts)\n","cmp('norm_logits', dnorm_logits, norm_logits)\n","cmp('logit_maxes', dlogit_maxes, logit_maxes)\n","cmp('logits', dlogits, logits)\n","cmp('h', dh, h)\n","cmp('W2', dW2, W2)\n","cmp('b2', db2, b2)\n","cmp('hpreact', dhpreact, hpreact)\n","cmp('bngain', dbngain, bngain)\n","cmp('bnbias', dbnbias, bnbias)\n","cmp('bnraw', dbnraw, bnraw)\n","cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n","cmp('bnvar', dbnvar, bnvar)\n","cmp('bndiff2', dbndiff2, bndiff2)\n","cmp('bndiff', dbndiff, bndiff)\n","cmp('bnmeani', dbnmeani, bnmeani)\n","cmp('hprebn', dhprebn, hprebn)\n","cmp('embcat', dembcat, embcat)\n","cmp('W1', dW1, W1)\n","cmp('b1', db1, b1)\n","cmp('emb', demb, emb)\n","cmp('C', dC, C)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRGVkRiW5oJR","executionInfo":{"status":"ok","timestamp":1753046574206,"user_tz":240,"elapsed":170,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"ca00051c-e0ea-4076-c3bd-6a5757b4f8cf"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------\n","torch.Size([32, 3, 10]) torch.Size([27, 10]) torch.Size([32, 3])\n","----------------------------\n","logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n","probs           | exact: True  | approximate: True  | maxdiff: 0.0\n","counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n","counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n","counts          | exact: True  | approximate: True  | maxdiff: 0.0\n","norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n","logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n","logits          | exact: True  | approximate: True  | maxdiff: 0.0\n","h               | exact: True  | approximate: True  | maxdiff: 0.0\n","W2              | exact: True  | approximate: True  | maxdiff: 0.0\n","b2              | exact: True  | approximate: True  | maxdiff: 0.0\n","hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","bnbias          | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n","bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n","bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n","bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n","bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n","bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","bnmeani         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","hprebn          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-10\n","embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","W1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n","b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n","emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","C               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"]}]},{"cell_type":"code","source":["# Exercise 2: backprop through cross_entropy but all in one go\n","# to complete this challenge look at the mathematical expression of the loss,\n","# take the derivative, simplify the expression, and just write it out\n","\n","# forward pass\n","\n","# before:\n","# logit_maxes = logits.max(1, keepdim=True).values\n","# norm_logits = logits - logit_maxes # subtract max for numerical stability\n","# counts = norm_logits.exp()\n","# counts_sum = counts.sum(1, keepdims=True)\n","# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","# probs = counts * counts_sum_inv\n","# logprobs = probs.log()\n","# loss = -logprobs[range(n), Yb].mean()\n","\n","# now:\n","loss_fast = F.cross_entropy(logits, Yb)\n","print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoynQGd6uBEI","executionInfo":{"status":"ok","timestamp":1753046717069,"user_tz":240,"elapsed":23,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"d33878c1-8f13-4a46-e483-c1a9ff478c0c"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["3.326474666595459 diff: -2.384185791015625e-07\n"]}]},{"cell_type":"code","source":["# backward pass\n","# TODO: need to study derivatives and recavive math skills to do so! ^_0\n","# -----------------\n","# YOUR CODE HERE :)\n","# dlogits = None # TODO. my solution is 3 lines\n","dlogits = F.softmax(logits, 1)\n","dlogits[range(n), Yb] -= 1\n","dlogits /= n\n","# -----------------\n","\n","cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPIeEAFPuDUC","executionInfo":{"status":"ok","timestamp":1753047068296,"user_tz":240,"elapsed":12,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"68df50f3-8389-448e-c2d7-585825b362a4"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["logits          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-09\n"]}]},{"cell_type":"code","source":["# Exercise 3: backprop through batchnorm but all in one go\n","# to complete this challenge look at the mathematical expression of the output of batchnorm,\n","# take the derivative w.r.t. its input, simplify the expression, and just write it out\n","# BatchNorm paper: https://arxiv.org/abs/1502.03167\n","\n","# forward pass\n","\n","# before:\n","# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","# bndiff = hprebn - bnmeani\n","# bndiff2 = bndiff**2\n","# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","# bnvar_inv = (bnvar + 1e-5)**-0.5\n","# bnraw = bndiff * bnvar_inv\n","# hpreact = bngain * bnraw + bnbias\n","\n","# now:\n","hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n","print('max diff:', (hpreact_fast - hpreact).abs().max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLaahhsnuE_F","executionInfo":{"status":"ok","timestamp":1753047634301,"user_tz":240,"elapsed":48,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"d2aff9da-1340-49d5-93db-6d88b12adfe9"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"]}]},{"cell_type":"code","source":["# backward pass\n","\n","# before we had:\n","# dbnraw = bngain * dhpreact\n","# dbndiff = bnvar_inv * dbnraw\n","# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n","# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n","# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n","# dbndiff += (2*bndiff) * dbndiff2\n","# dhprebn = dbndiff.clone()\n","# dbnmeani = (-dbndiff).sum(0)\n","# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n","\n","# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n","# (you'll also need to use some of the variables from the forward pass up above)\n","\n","# -----------------\n","# TODO: need to study derivatives and recavive math skills to do so! ^_0\n","# complicated!\n","# YOUR CODE HERE :)\n","# dhprebn = None # TODO. my solution is 1 (long) line\n","dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n","# -----------------\n","\n","cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQeBZ23vuGg1","executionInfo":{"status":"ok","timestamp":1753048720878,"user_tz":240,"elapsed":16,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"efc5603e-ef87-4436-f7ec-5bf860490b4a"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"]}]},{"cell_type":"code","source":["# Exercise 4: putting it all together!\n","# Train the MLP neural net with your own backward pass\n","\n","# init\n","n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True\n","\n","# same optimization as last time\n","max_steps = 200000\n","batch_size = 32\n","n = batch_size # convenience\n","lossi = []\n","\n","# use this context manager for efficiency once your backward pass is written (TODO)\n","#with torch.no_grad():\n","\n","# kick off optimization\n","for i in range(max_steps):\n","\n","  # minibatch construct\n","  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n","\n","  # forward pass\n","  emb = C[Xb] # embed the characters into vectors\n","  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","  # Linear layer\n","  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n","  # BatchNorm layer\n","  # -------------------------------------------------------------\n","  bnmean = hprebn.mean(0, keepdim=True)\n","  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n","  bnvar_inv = (bnvar + 1e-5)**-0.5\n","  bnraw = (hprebn - bnmean) * bnvar_inv\n","  hpreact = bngain * bnraw + bnbias\n","  # -------------------------------------------------------------\n","  # Non-linearity\n","  h = torch.tanh(hpreact) # hidden layer\n","  logits = h @ W2 + b2 # output layer\n","  loss = F.cross_entropy(logits, Yb) # loss function\n","\n","  # backward pass\n","  for p in parameters:\n","    p.grad = None\n","  # loss.backward() # use this for correctness comparisons, delete it later!\n","\n","  # manual backprop! #swole_doge_meme\n","  # -----------------\n","  # YOUR CODE HERE :)\n","  #dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n","  dlogits = F.softmax(logits, 1)\n","  dlogits[range(n), Yb] -= 1\n","  dlogits /= n\n","  # 2nd layer backprop\n","  dh = dlogits @ W2.T\n","  dW2 = h.T @ dlogits\n","  db2 = dlogits.sum(0)\n","  # tanh\n","  dhpreact = (1.0 - h**2) * dh\n","  # batchnorm backprop\n","  dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n","  dbnbias = (dhpreact).sum(0, keepdim=True)\n","  dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n","  # 1st layer\n","  dembcat = dhprebn @ W1.T\n","  dW1 = embcat.T @ dhprebn\n","  db1 = dhprebn.sum(0)\n","  # embedding\n","  demb = dembcat.view(emb.shape)\n","  dC = torch.zeros_like(C)\n","  for k in range(Xb.shape[0]):\n","    for j in range(Xb.shape[1]):\n","      ix = Xb[k,j]\n","      dC[ix] += demb[k,j]\n","  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n","  # -----------------\n","\n","  # update\n","  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n","  for p, grad in zip(parameters, grads):\n","    # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n","    p.data += -lr * grad # new way of swole doge TODO: enable\n","\n","  # track stats\n","  if i % 10000 == 0: # print every once in a while\n","    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","  lossi.append(loss.log10().item())\n","\n","  #if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n","  #  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lv-RelwWuISl","executionInfo":{"status":"ok","timestamp":1753050602361,"user_tz":240,"elapsed":1182461,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"5ac5cd27-68a3-44ed-efc4-c5ac7efc3081"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["12297\n","      0/ 200000: 3.8041\n","  10000/ 200000: 2.2126\n","  20000/ 200000: 2.3834\n","  30000/ 200000: 2.4811\n","  40000/ 200000: 2.0072\n","  50000/ 200000: 2.3778\n","  60000/ 200000: 2.4355\n","  70000/ 200000: 2.0452\n","  80000/ 200000: 2.3977\n","  90000/ 200000: 2.2220\n"," 100000/ 200000: 2.0284\n"," 110000/ 200000: 2.2783\n"," 120000/ 200000: 1.9599\n"," 130000/ 200000: 2.4079\n"," 140000/ 200000: 2.2610\n"," 150000/ 200000: 2.1705\n"," 160000/ 200000: 1.9020\n"," 170000/ 200000: 1.8692\n"," 180000/ 200000: 2.0895\n"," 190000/ 200000: 1.9525\n"]}]},{"cell_type":"code","source":["# useful for checking your gradients\n","# for p,g in zip(parameters, grads):\n","#  cmp(str(tuple(p.shape)), g, p)"],"metadata":{"id":"Vy89dVVeuJvT","executionInfo":{"status":"ok","timestamp":1753053747743,"user_tz":240,"elapsed":39,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# calibrate the batch norm at the end of training\n","\n","with torch.no_grad():\n","  # pass the training set through\n","  emb = C[Xtr]\n","  embcat = emb.view(emb.shape[0], -1)\n","  hpreact = embcat @ W1 + b1\n","  # measure the mean/std over the entire training set\n","  bnmean = hpreact.mean(0, keepdim=True)\n","  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"],"metadata":{"id":"i-0PjfaWuKtP","executionInfo":{"status":"ok","timestamp":1753053750801,"user_tz":240,"elapsed":739,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# evaluate train and val loss\n","\n","@torch.no_grad() # this decorator disables gradient tracking\n","def split_loss(split):\n","  x,y = {\n","    'train': (Xtr, Ytr),\n","    'val': (Xdev, Ydev),\n","    'test': (Xte, Yte),\n","  }[split]\n","  emb = C[x] # (N, block_size, n_embd)\n","  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","  hpreact = embcat @ W1 + b1\n","  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","  h = torch.tanh(hpreact) # (N, n_hidden)\n","  logits = h @ W2 + b2 # (N, vocab_size)\n","  loss = F.cross_entropy(logits, y)\n","  print(split, loss.item())\n","\n","split_loss('train')\n","split_loss('val')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6Z4omt0uMJh","executionInfo":{"status":"ok","timestamp":1753053757127,"user_tz":240,"elapsed":1037,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"764629b9-1f8e-4c57-9a0b-a9df172a92cf"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["train 2.0722198486328125\n","val 2.110687255859375\n"]}]},{"cell_type":"code","source":["# I achieved:\n","# train 2.0718822479248047\n","# val 2.1162495613098145"],"metadata":{"id":"jHUa-G5EuNaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample from the model\n","g = torch.Generator().manual_seed(2147483647 + 10)\n","\n","for _ in range(20):\n","\n","    out = []\n","    context = [0] * block_size # initialize with all ...\n","    while True:\n","      # forward pass\n","      emb = C[torch.tensor([context])] # (1,block_size,d)\n","      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","      hpreact = embcat @ W1 + b1\n","      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","      h = torch.tanh(hpreact) # (N, n_hidden)\n","      logits = h @ W2 + b2 # (N, vocab_size)\n","      # sample\n","      probs = F.softmax(logits, dim=1)\n","      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n","      context = context[1:] + [ix]\n","      out.append(ix)\n","      if ix == 0:\n","        break\n","\n","    print(''.join(itos[i] for i in out))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xb1am1G9uORn","executionInfo":{"status":"ok","timestamp":1753053761133,"user_tz":240,"elapsed":57,"user":{"displayName":"Thierry Aubin","userId":"15501859295335027249"}},"outputId":"0d2a6b5a-e35c-4d14-9e01-c457c1db2a82"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["mora.\n","mayah.\n","see.\n","mad.\n","rylle.\n","emman.\n","endraeg.\n","adelynnelin.\n","shi.\n","jenleigh.\n","estanaraelynn.\n","hokalin.\n","shravrishirael.\n","kindreelynn.\n","novana.\n","ububacder.\n","yarul.\n","els.\n","kayshaston.\n","mahil.\n"]}]}]}